{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand-Written English Character Recognition System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "- We are classifying 0-9 digit images into one of the 10 classes from 0-9.\n",
    "- For this, we are using mnist dataset.\n",
    "- Mnist dataset contains 42000 data for training and 28000 for testing.\n",
    "- It contains images of 28x28 pixels.\n",
    "- It is a multiclass classification problem where number of classes is 10.\n",
    "- We are using convolutional neural network with our own specific architecture which we are building from scratch.\n",
    "- We have also used image preprocessing for better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing important packages:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten,Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#setting GPU Configuration\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = .25\n",
    "\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading (train_data, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 91395 images belonging to 66 classes.\n",
      "Found 22824 images belonging to 66 classes.\n"
     ]
    }
   ],
   "source": [
    "num_classes = 66\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    validation_split=0.2)\n",
    "\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'trainingSet/',\n",
    "        target_size=(64, 64),\n",
    "        color_mode=\"grayscale\",\n",
    "        batch_size=32,\n",
    "        class_mode='categorical',\n",
    "        subset='training') # set as training data\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    'trainingSet/', # same directory as training data\n",
    "    target_size=(64, 64),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size = 32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation') # set as validation data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#': 0,\n",
       " '$': 1,\n",
       " '&': 2,\n",
       " '0__': 3,\n",
       " '1': 4,\n",
       " '2': 5,\n",
       " '3': 6,\n",
       " '4': 7,\n",
       " '5': 8,\n",
       " '6': 9,\n",
       " '7': 10,\n",
       " '8': 11,\n",
       " '9': 12,\n",
       " '@': 13,\n",
       " 'A': 14,\n",
       " 'B': 15,\n",
       " 'C': 16,\n",
       " 'D': 17,\n",
       " 'E': 18,\n",
       " 'F': 19,\n",
       " 'G': 20,\n",
       " 'H': 21,\n",
       " 'J': 22,\n",
       " 'K': 23,\n",
       " 'L': 24,\n",
       " 'M': 25,\n",
       " 'N': 26,\n",
       " 'O': 27,\n",
       " 'P': 28,\n",
       " 'Q': 29,\n",
       " 'R': 30,\n",
       " 'S': 31,\n",
       " 'T': 32,\n",
       " 'U': 33,\n",
       " 'V': 34,\n",
       " 'W': 35,\n",
       " 'X': 36,\n",
       " 'Y': 37,\n",
       " 'Z': 38,\n",
       " 'a_': 39,\n",
       " 'b_': 40,\n",
       " 'c_': 41,\n",
       " 'd_': 42,\n",
       " 'e_': 43,\n",
       " 'f__': 44,\n",
       " 'g___': 45,\n",
       " 'h_': 46,\n",
       " 'i_': 47,\n",
       " 'i__': 48,\n",
       " 'j_': 49,\n",
       " 'k_': 50,\n",
       " 'l_': 51,\n",
       " 'm_': 52,\n",
       " 'n_': 53,\n",
       " 'o_': 54,\n",
       " 'p___': 55,\n",
       " 'q_': 56,\n",
       " 'r_': 57,\n",
       " 's_': 58,\n",
       " 't_': 59,\n",
       " 'u_': 60,\n",
       " 'v_': 61,\n",
       " 'w_': 62,\n",
       " 'x_': 63,\n",
       " 'y__': 64,\n",
       " 'z_': 65}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_class_labels = {v: k for k, v in class_labels.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '#',\n",
       " 1: '$',\n",
       " 2: '&',\n",
       " 3: '0__',\n",
       " 4: '1',\n",
       " 5: '2',\n",
       " 6: '3',\n",
       " 7: '4',\n",
       " 8: '5',\n",
       " 9: '6',\n",
       " 10: '7',\n",
       " 11: '8',\n",
       " 12: '9',\n",
       " 13: '@',\n",
       " 14: 'A',\n",
       " 15: 'B',\n",
       " 16: 'C',\n",
       " 17: 'D',\n",
       " 18: 'E',\n",
       " 19: 'F',\n",
       " 20: 'G',\n",
       " 21: 'H',\n",
       " 22: 'J',\n",
       " 23: 'K',\n",
       " 24: 'L',\n",
       " 25: 'M',\n",
       " 26: 'N',\n",
       " 27: 'O',\n",
       " 28: 'P',\n",
       " 29: 'Q',\n",
       " 30: 'R',\n",
       " 31: 'S',\n",
       " 32: 'T',\n",
       " 33: 'U',\n",
       " 34: 'V',\n",
       " 35: 'W',\n",
       " 36: 'X',\n",
       " 37: 'Y',\n",
       " 38: 'Z',\n",
       " 39: 'a_',\n",
       " 40: 'b_',\n",
       " 41: 'c_',\n",
       " 42: 'd_',\n",
       " 43: 'e_',\n",
       " 44: 'f__',\n",
       " 45: 'g___',\n",
       " 46: 'h_',\n",
       " 47: 'i_',\n",
       " 48: 'i__',\n",
       " 49: 'j_',\n",
       " 50: 'k_',\n",
       " 51: 'l_',\n",
       " 52: 'm_',\n",
       " 53: 'n_',\n",
       " 54: 'o_',\n",
       " 55: 'p___',\n",
       " 56: 'q_',\n",
       " 57: 'r_',\n",
       " 58: 's_',\n",
       " 59: 't_',\n",
       " 60: 'u_',\n",
       " 61: 'v_',\n",
       " 62: 'w_',\n",
       " 63: 'x_',\n",
       " 64: 'y__',\n",
       " 65: 'z_'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 62, 62, 128)       1280      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 62, 62, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 60, 60, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 256)       590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 128)       295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 64)          73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 66)                8514      \n",
      "=================================================================\n",
      "Total params: 1,365,890\n",
      "Trainable params: 1,364,226\n",
      "Non-trainable params: 1,664\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (64,64,1)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#input Layer\n",
    "model.add(Conv2D(128, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,kernel_initializer='he_normal'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#hidden Layer 1\n",
    "model.add(Conv2D(256, (3, 3), activation='relu',kernel_initializer='he_normal'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#hidden Layer 2\n",
    "model.add(Conv2D(256, (3, 3), activation='relu',kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#hidden Layer 3\n",
    "model.add(Conv2D(128, (3, 3), activation='relu',kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#hidden Layer 3\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#Dense Layer 1\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu',kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#Dense Layer 2\n",
    "model.add(Dense(128, activation='relu',kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#Output layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compiling and Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2860/2860 [==============================] - 748s 261ms/step - loss: 1.7648 - accuracy: 0.5032 - val_loss: 11.4312 - val_accuracy: 0.0323\n",
      "Epoch 2/20\n",
      "2860/2860 [==============================] - 741s 259ms/step - loss: 0.7506 - accuracy: 0.7390 - val_loss: 3.3292 - val_accuracy: 0.2542\n",
      "Epoch 3/20\n",
      "2860/2860 [==============================] - 744s 260ms/step - loss: 0.6238 - accuracy: 0.7782 - val_loss: 1.1445 - val_accuracy: 0.6513\n",
      "Epoch 4/20\n",
      "2860/2860 [==============================] - 743s 260ms/step - loss: 0.5584 - accuracy: 0.7987 - val_loss: 1.2076 - val_accuracy: 0.6433\n",
      "Epoch 5/20\n",
      "2860/2860 [==============================] - 742s 260ms/step - loss: 0.5141 - accuracy: 0.8120 - val_loss: 0.8646 - val_accuracy: 0.7139\n",
      "Epoch 6/20\n",
      "2860/2860 [==============================] - 743s 260ms/step - loss: 0.4841 - accuracy: 0.8227 - val_loss: 0.6407 - val_accuracy: 0.7949\n",
      "Epoch 7/20\n",
      "2860/2860 [==============================] - 741s 259ms/step - loss: 0.4511 - accuracy: 0.8327 - val_loss: 0.5285 - val_accuracy: 0.8287\n",
      "Epoch 8/20\n",
      "2860/2860 [==============================] - 736s 257ms/step - loss: 0.4328 - accuracy: 0.8377 - val_loss: 2.2997 - val_accuracy: 0.5165\n",
      "Epoch 9/20\n",
      "2860/2860 [==============================] - 734s 257ms/step - loss: 0.4123 - accuracy: 0.8455 - val_loss: 0.5202 - val_accuracy: 0.8345\n",
      "Epoch 10/20\n",
      "2860/2860 [==============================] - 746s 261ms/step - loss: 0.4007 - accuracy: 0.8500 - val_loss: 0.9604 - val_accuracy: 0.7003\n",
      "Epoch 11/20\n",
      "2860/2860 [==============================] - 745s 260ms/step - loss: 0.3856 - accuracy: 0.8544 - val_loss: 0.6645 - val_accuracy: 0.7824\n",
      "Epoch 12/20\n",
      "2860/2860 [==============================] - 765s 267ms/step - loss: 0.3723 - accuracy: 0.8606 - val_loss: 0.5449 - val_accuracy: 0.8206\n",
      "Epoch 13/20\n",
      "2860/2860 [==============================] - 741s 259ms/step - loss: 0.3640 - accuracy: 0.8624 - val_loss: 0.5078 - val_accuracy: 0.8416\n",
      "Epoch 14/20\n",
      "2860/2860 [==============================] - 758s 265ms/step - loss: 0.3513 - accuracy: 0.8655 - val_loss: 0.8064 - val_accuracy: 0.7368\n",
      "Epoch 15/20\n",
      "2860/2860 [==============================] - 784s 274ms/step - loss: 0.3463 - accuracy: 0.8684 - val_loss: 1.5045 - val_accuracy: 0.6409\n",
      "Epoch 16/20\n",
      "2860/2860 [==============================] - 775s 271ms/step - loss: 0.3341 - accuracy: 0.8711 - val_loss: 0.6516 - val_accuracy: 0.8021\n",
      "Epoch 17/20\n",
      "2860/2860 [==============================] - 776s 271ms/step - loss: 0.3303 - accuracy: 0.8730 - val_loss: 0.6496 - val_accuracy: 0.8060\n",
      "Epoch 18/20\n",
      "2860/2860 [==============================] - 771s 270ms/step - loss: 0.3212 - accuracy: 0.8755 - val_loss: 0.6002 - val_accuracy: 0.8228\n",
      "Epoch 19/20\n",
      "2860/2860 [==============================] - 741s 259ms/step - loss: 0.3153 - accuracy: 0.8784 - val_loss: 0.7682 - val_accuracy: 0.7609\n",
      "Epoch 20/20\n",
      "2860/2860 [==============================] - 738s 258ms/step - loss: 0.3082 - accuracy: 0.8815 - val_loss: 0.6895 - val_accuracy: 0.7884\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch = train_generator.samples //32,\n",
    "        validation_data = validation_generator, \n",
    "        validation_steps = validation_generator.samples // 32,\n",
    "        epochs=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving the model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "save_model = model.save(\"model_epoch20.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Real World Data - Handwritten and Picture Taken from Mobile Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model_epoch20.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_class_labels={0: '#',\n",
    " 1: '$',\n",
    " 2: '0__',\n",
    " 3: '1',\n",
    " 4: '2',\n",
    " 5: '3',\n",
    " 6: '4',\n",
    " 7: '5',\n",
    " 8: '6',\n",
    " 9: '7',\n",
    " 10: '8',\n",
    " 11: '9',\n",
    " 12: '@',\n",
    " 13: 'A',\n",
    " 14: 'B',\n",
    " 15: 'C',\n",
    " 16: 'D',\n",
    " 17: 'E',\n",
    " 18: 'F',\n",
    " 19: 'G',\n",
    " 20: 'H',\n",
    " 21: 'J',\n",
    " 22: 'K',\n",
    " 23: 'L',\n",
    " 24: 'M',\n",
    " 25: 'N',\n",
    " 26: 'O',\n",
    " 27: 'P',\n",
    " 28: 'Q',\n",
    " 29: 'R',\n",
    " 30: 'S',\n",
    " 31: 'T',\n",
    " 32: 'U',\n",
    " 33: 'V',\n",
    " 34: 'W',\n",
    " 35: 'X',\n",
    " 36: 'Y',\n",
    " 37: 'Z',\n",
    " 38: '&',\n",
    " 39: 'a_',\n",
    " 40: 'b_',\n",
    " 41: 'c_',\n",
    " 42: 'd_',\n",
    " 43: 'e_',\n",
    " 44: 'f__',\n",
    " 45: 'g___',\n",
    " 46: 'h_',\n",
    " 47: 'i_',\n",
    " 48: 'i__',\n",
    " 49: 'j_',\n",
    " 50: 'k_',\n",
    " 51: 'l_',\n",
    " 52: 'm_',\n",
    " 53: 'n_',\n",
    " 54: 'o_',\n",
    " 55: 'p___',\n",
    " 56: 'q_',\n",
    " 57: 'r_',\n",
    " 58: 's_',\n",
    " 59: 't_',\n",
    " 60: 'u_',\n",
    " 61: 'v_',\n",
    " 62: 'w_',\n",
    " 63: 'x_',\n",
    " 64: 'y__',\n",
    " 65: 'z_'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data Preparation\n",
    "def image_preprocessing(file_path, count):\n",
    "    img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE) # reading in grayscale\n",
    "    th, im_th = cv2.threshold(img,  128, 255, cv2.THRESH_OTSU)\n",
    "    im_resize = cv2.resize(im_th, (64,64),interpolation = cv2.INTER_NEAREST)\n",
    "    cv2.imwrite(\"testSet2//test_{}.png\".format(count),im_resize)\n",
    "    \n",
    "\n",
    "count  = 0\n",
    "for img in os.listdir('testSet'):\n",
    "    count += 1\n",
    "    image_preprocessing(\"testSet/\" + img, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on Test Data\n",
    "res = []\n",
    "data = []\n",
    "img_name = []\n",
    "for img in os.listdir('test_T'):\n",
    "\n",
    "    image = cv2.imread('test_T/{}'.format(img),cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image , (64,64))\n",
    "    image = image.reshape(64,64,1)\n",
    "    image = image/255\n",
    "    \n",
    "    data.append(image)\n",
    "    \n",
    "    img_name.append(img)\n",
    "    \n",
    "    if len(data) % 27 == 0:\n",
    "        pred = model.predict_on_batch(np.array(data))\n",
    "        \n",
    "        for p in pred:\n",
    "            res.append(rev_class_labels[np.argmax(p)])\n",
    "            \n",
    "        data = []\n",
    "#         res.append([img,*list(np.argmax(pred,axis=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.56129409e-09, 8.83476292e-10, 6.81118991e-07, 2.39680142e-09,\n",
       "       6.24459107e-09, 3.12312091e-06, 1.13523482e-13, 2.75805583e-08,\n",
       "       2.32515305e-08, 2.27992798e-12, 1.72491473e-05, 4.90362140e-10,\n",
       "       7.71111172e-06, 2.84347522e-07, 9.96804476e-01, 1.21080235e-09,\n",
       "       1.49140495e-03, 2.77351244e-08, 3.81823240e-09, 3.61283287e-06,\n",
       "       1.18091767e-08, 6.65308164e-08, 4.16847783e-08, 1.44619250e-08,\n",
       "       5.27809751e-09, 7.91608931e-11, 5.10955215e-06, 1.42024892e-05,\n",
       "       1.23916072e-06, 1.45991052e-07, 1.10359645e-06, 1.26534148e-11,\n",
       "       2.32232665e-07, 4.04574827e-08, 6.74055158e-08, 1.50717057e-08,\n",
       "       1.24251551e-08, 1.58438553e-07, 9.03679021e-10, 4.51192790e-07,\n",
       "       1.61985902e-03, 1.83456913e-11, 3.62233408e-08, 7.80729579e-07,\n",
       "       3.67083053e-09, 1.28716802e-05, 1.37651424e-09, 8.03480837e-09,\n",
       "       2.29629076e-11, 1.25299655e-08, 1.01129778e-07, 2.34843665e-08,\n",
       "       7.38616102e-09, 9.51171253e-10, 5.83278825e-06, 4.93392508e-06,\n",
       "       3.23030213e-06, 8.62026353e-11, 6.05625630e-08, 3.48501616e-10,\n",
       "       1.94987959e-09, 8.35296332e-10, 1.92223992e-09, 9.79086057e-09,\n",
       "       7.55287601e-08, 6.57312967e-07], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for name, r in zip(img_name, res):\n",
    "    result.append([name, r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.array(result)\n",
    "result_df = pd.DataFrame(result,columns = ['Image Name','Predicted_Category'])\n",
    "\n",
    "# saving the result in csv format\n",
    "export_result = result_df.to_csv('result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "still lot of error\n",
    " we will test on batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on real_world data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing(file_name):\n",
    "    name = file_name.split('.')[0]\n",
    "    alpha,beta,th = d[name]\n",
    "    img = cv2.imread('test_real/{}'.format(file_name),cv2.IMREAD_GRAYSCALE) # reading in grayscale\n",
    "    new_img = cv2.resize(img,(28,28)) # resizing it to (28x28) matrix\n",
    "\n",
    "    new_img = alpha*new_img + beta # changing contrast\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            if new_img[i][j] < th:\n",
    "                new_img[i][j] = 0\n",
    "            else:\n",
    "                new_img[i][j] = 255\n",
    "    plt.imshow(new_img)\n",
    "    plt.show()\n",
    "    return (new_img.reshape(1,28,28,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing on real data and saving it to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for img in os.listdir('test_real'):\n",
    "    print(img)\n",
    "    final_img = image_preprocessing(img)\n",
    "    pred = model.predict(final_img)\n",
    "    res.append([img,*list(np.argmax(pred,axis=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.array(res)\n",
    "result_df = pd.DataFrame(res,columns = ['Name','Predicted_class'])\n",
    "\n",
    "# saving the result in csv format\n",
    "export_result = result_df.to_csv('result_real.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on Sequences of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('seq.jpg',cv2.IMREAD_GRAYSCALE) # reading in grayscale\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocessing2(img):\n",
    "    alpha,beta,th = 3, 170, 110\n",
    "    new_img = cv2.resize(img,(28,28)) # resizing it to (28x28) matrix\n",
    "    new_img = alpha*new_img + beta # changing contrast\n",
    "    for i in range(28):\n",
    "        for j in range(28):\n",
    "            if new_img[i][j] < th:\n",
    "                new_img[i][j] = 0\n",
    "            else:\n",
    "                new_img[i][j] = 255\n",
    "    plt.imshow(new_img)\n",
    "    plt.show()\n",
    "    return (new_img.reshape(1,28,28,1))\n",
    "\n",
    "\n",
    "def thresh_callback(file_name):\n",
    "    thresh = 127\n",
    "    max_thresh = 255\n",
    "    img = cv2.imread(file_name)\n",
    "    img = cv2.resize(img,(450,450))\n",
    "    \n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    src_gray = cv2.blur(img_gray, (3,3))\n",
    "    \n",
    "    \n",
    "    canny_output = cv2.Canny(src_gray, thresh, max_thresh)\n",
    "    \n",
    "    \n",
    "    _,contours,_ = cv2.findContours(canny_output, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    \n",
    "    contours_poly = [None]*len(contours)\n",
    "    boundRect = []\n",
    "    for i, c in enumerate(contours):\n",
    "        if cv2.contourArea(c) > 300:\n",
    "            contours_poly[i] = cv2.approxPolyDP(c, 1, False)\n",
    "            boundRect.append(cv2.boundingRect(contours_poly[i]))\n",
    "        \n",
    "    drawing = img\n",
    "    \n",
    "    \n",
    "    for i in range(len(boundRect)):\n",
    "        color = (255,0,0)\n",
    "        cv2.rectangle(drawing, (int(boundRect[i][0]), int(boundRect[i][1])), \\\n",
    "          (int(boundRect[i][0]+boundRect[i][2]), int(boundRect[i][1]+boundRect[i][3])), color, 2)\n",
    "    plt.figure(figsize=(14,8))\n",
    "    plt.imshow(drawing)\n",
    "    plt.show()\n",
    "    s = list(set(boundRect))\n",
    "    for i in range(len(s)):\n",
    "        crop_img = img_gray[s[i][1]:s[i][1]+s[i][3] ,s[i][0]:s[i][0]+s[i][2]]\n",
    "#         plt.imshow(crop_img)\n",
    "#         plt.show()\n",
    "        final_img = image_preprocessing2(crop_img)\n",
    "        predicted_value = model.predict(final_img)\n",
    "        print(predicted_value)\n",
    "        print(\"Predicted Result is: \",*list(np.argmax(predicted_value,axis=1)))\n",
    "        \n",
    "#         file_name = 'data/{}.jpeg'.format(str(i))\n",
    "#         cv2.imwrite(file_name,crop_img)\n",
    "    return drawing\n",
    "    \n",
    "\n",
    "drawing = thresh_callback('seq.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
